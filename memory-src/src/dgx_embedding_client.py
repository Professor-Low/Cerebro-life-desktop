"""
GPU Embedding Service Client

Provides async HTTP client for the GPU-based embedding service.
Falls back gracefully if GPU server is unavailable.

Usage:
    from dgx_embedding_client import dgx_embed, is_dgx_embedding_available

    # Check if GPU server is available
    if await is_dgx_embedding_available():
        vectors = await dgx_embed(["text1", "text2"], batch_size=128)
        # vectors is np.ndarray shape (2, 768)
    else:
        # Fall back to local embedding or keyword search
        ...
"""

import asyncio
import json
import os
import socket
from typing import List, Optional
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

import numpy as np

# Configuration
DGX_HOST = os.environ.get("DGX_EMBEDDING_HOST", os.environ.get("CEREBRO_DGX_HOST", ""))
DGX_PORT = int(os.environ.get("DGX_EMBEDDING_PORT", "8781"))
DGX_TIMEOUT = float(os.environ.get("DGX_EMBEDDING_TIMEOUT", "10.0"))

DGX_EMBEDDING_URL = f"http://{DGX_HOST}:{DGX_PORT}"

# Cache for GPU server availability (avoid repeated health checks)
_dgx_available: Optional[bool] = None

# If no GPU host configured, mark as unavailable immediately
if not DGX_HOST:
    _dgx_available = False
_dgx_check_time: float = 0
_DGX_CHECK_INTERVAL = 30  # Re-check every 30 seconds


def _is_dgx_reachable() -> bool:
    """Quick socket check if GPU server is reachable"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(1.0)
        result = sock.connect_ex((DGX_HOST, DGX_PORT))
        sock.close()
        return result == 0
    except Exception:
        return False


async def is_dgx_embedding_available() -> bool:
    """
    Check if GPU embedding service is available.
    Caches result for _DGX_CHECK_INTERVAL seconds.
    """
    global _dgx_available, _dgx_check_time
    import time

    now = time.time()
    if _dgx_available is not None and (now - _dgx_check_time) < _DGX_CHECK_INTERVAL:
        return _dgx_available

    # Run socket check in thread pool
    loop = asyncio.get_event_loop()
    try:
        reachable = await asyncio.wait_for(
            loop.run_in_executor(None, _is_dgx_reachable),
            timeout=2.0
        )

        if not reachable:
            _dgx_available = False
            _dgx_check_time = now
            return False

        # Try health endpoint
        available = await asyncio.wait_for(
            loop.run_in_executor(None, _check_health),
            timeout=3.0
        )

        _dgx_available = available
        _dgx_check_time = now
        return available

    except asyncio.TimeoutError:
        _dgx_available = False
        _dgx_check_time = now
        return False
    except Exception:
        _dgx_available = False
        _dgx_check_time = now
        return False


def _check_health() -> bool:
    """Blocking health check"""
    try:
        req = Request(f"{DGX_EMBEDDING_URL}/health")
        req.add_header("Accept", "application/json")
        with urlopen(req, timeout=DGX_TIMEOUT) as response:
            if response.status == 200:
                data = json.loads(response.read())
                return data.get("status") == "healthy"
        return False
    except Exception:
        return False


def _do_embed(texts: List[str], batch_size: int = 128, normalize: bool = True) -> Optional[np.ndarray]:
    """Blocking embed call"""
    try:
        payload = json.dumps({
            "texts": texts,
            "batch_size": batch_size,
            "normalize": normalize
        }).encode("utf-8")

        req = Request(
            f"{DGX_EMBEDDING_URL}/embed",
            data=payload,
            method="POST"
        )
        req.add_header("Content-Type", "application/json")
        req.add_header("Accept", "application/json")

        with urlopen(req, timeout=DGX_TIMEOUT) as response:
            if response.status == 200:
                data = json.loads(response.read())
                embeddings = data.get("embeddings", [])
                return np.array(embeddings, dtype=np.float32)
            else:
                return None

    except HTTPError as e:
        print(f"[GPU Embed] HTTP error: {e.code}")
        return None
    except URLError as e:
        print(f"[GPU Embed] Connection error: {e.reason}")
        return None
    except Exception as e:
        print(f"[GPU Embed] Error: {e}")
        return None


async def dgx_embed(
    texts: List[str],
    batch_size: int = 128,
    normalize: bool = True,
    timeout: float = None
) -> Optional[np.ndarray]:
    """
    Generate embeddings using GPU embedding service.

    Args:
        texts: List of strings to embed
        batch_size: Processing batch size (default 128, max 512)
        normalize: Whether to L2 normalize for cosine similarity (default True)
        timeout: Request timeout (default: DGX_TIMEOUT)

    Returns:
        Numpy array of embeddings (N x 768) or None if GPU server unavailable
    """
    if timeout is None:
        timeout = DGX_TIMEOUT

    if not texts:
        return np.array([], dtype=np.float32)

    loop = asyncio.get_event_loop()

    try:
        result = await asyncio.wait_for(
            loop.run_in_executor(None, lambda: _do_embed(texts, batch_size, normalize)),
            timeout=timeout
        )
        return result

    except asyncio.TimeoutError:
        print(f"[GPU Embed] Request timed out after {timeout}s")
        return None
    except Exception as e:
        print(f"[GPU Embed] Error: {e}")
        return None


async def dgx_embed_batch(
    texts: List[str],
    max_batch: int = 512,
    internal_batch: int = 128
) -> Optional[np.ndarray]:
    """
    Embed large number of texts by splitting into chunks.

    Args:
        texts: List of strings to embed
        max_batch: Maximum texts per API call (default 512)
        internal_batch: Batch size for GPU processing (default 128)

    Returns:
        Numpy array of all embeddings concatenated
    """
    if not texts:
        return np.array([], dtype=np.float32)

    all_embeddings = []

    for i in range(0, len(texts), max_batch):
        batch = texts[i:i+max_batch]
        embeddings = await dgx_embed(batch, batch_size=internal_batch)

        if embeddings is None:
            return None  # Fail fast if GPU server fails

        all_embeddings.append(embeddings)

    return np.vstack(all_embeddings) if all_embeddings else None


async def dgx_embedding_stats() -> Optional[dict]:
    """Get GPU embedding service stats"""
    loop = asyncio.get_event_loop()

    def _get_stats():
        try:
            req = Request(f"{DGX_EMBEDDING_URL}/stats")
            req.add_header("Accept", "application/json")
            with urlopen(req, timeout=DGX_TIMEOUT) as response:
                if response.status == 200:
                    return json.loads(response.read())
        except Exception:
            pass
        return None

    try:
        return await asyncio.wait_for(
            loop.run_in_executor(None, _get_stats),
            timeout=DGX_TIMEOUT
        )
    except asyncio.TimeoutError:
        return None


def invalidate_dgx_embedding_cache():
    """Force re-check of GPU server availability on next call"""
    global _dgx_available, _dgx_check_time
    _dgx_available = None
    _dgx_check_time = 0


# Synchronous wrappers for non-async code
def dgx_embed_sync(texts: List[str], batch_size: int = 128) -> Optional[np.ndarray]:
    """Synchronous version of dgx_embed"""
    return _do_embed(texts, batch_size)


def is_dgx_embedding_available_sync() -> bool:
    """Synchronous version of is_dgx_embedding_available"""
    return _is_dgx_reachable() and _check_health()
